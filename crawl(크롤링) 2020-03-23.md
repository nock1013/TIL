# crawl(크롤링) 2020-03-23



## - 1. N2H4

* 네이버 뉴스와 다음 뉴스의 댓글 가져오기

```R
#댓글(네이버??)
install.packages("N2H4")
library(N2H4)
library(stringr)
library(dplyr)
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
getAllComment(url) %>%
  select(userName,contents) ->mydata
mydata
mycomment <- mydata$contents  
mycomment
```

## -2. rvest

* html로 생성된 웹사이트의 경우 rvest패키지로 스크래핑이 가능하다.
* css의 선택자를 활용할 수 있는 방법 - rvest
* 

```R
#css의 선택자를 활용할 수 있는 방법 - rvest
install.packages("rvest")
library(rvest)
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
readPage <-  read_html(url)
#nodes : 태그 여러개를 뽑아온다.?? .subject_fixed 은 클래스명
readPage %>% 

html_nodes(선택자) : 일치하는 모든 태그를 추출 ex)tag 나 class로 모든 요소를 추출

html_node(선택자) : 일치하는 태그 한개 ex)id를 찾는경우 사용

html_text() : 텍스트를 추출

html_name() : attributes의 이름 추출

html_children() : 하위 노드 추출

html_attr() : attribute추출

  html_nodes("span.subject_fixed") %>% 
  html_text() ->title_data
title_data
```

## - 3. KoNLP

